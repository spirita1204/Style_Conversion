from django.shortcuts import render
from django.views.generic import View
from django.http import HttpResponse
from .forms import UserSongForm
import os
from django.http import HttpResponseRedirect
 # from django's docs
#from django.core.files.storage import FileSystemStorage

###edward import start###
import imageio
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage.transform import resize
#import demo
from video_exfile_face.demo import load_checkpoints
from video_exfile_face.demo import make_animation
from skimage import img_as_ubyte

import moviepy.editor as moviepy
###edward import end###

from .models import UserSong
def handle_uploaded_file(f): #存到本機端 重複命名 還要改
    ext = os.path.splitext(f.name)[1]
    destination = open('file/name%s'%(ext), 'wb+')
    for chunk in f.chunks():
        destination.write(chunk)
    destination.close()
# ns used
import noisereduce as nr
from noisereduce.generate_noise import band_limited_noise
#
import soundfile as sf
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.optimizers import Adam
import tensorflow.keras.backend as K
import numpy as np
import matplotlib.pyplot as plt
import librosa
import os
import IPython
import tensorflow as tf
import librosa
import ffmpeg #支援多format 避免no backend error 
#@title Hyperparameters
#Hyperparameters

hop=192               #hop size (window size = 6*hop)
sr=16000              #sampling rate
min_level_db=-100     #reference values to normalize data
ref_level_db=20

shape=24              #length of time axis of split specrograms to feed to generator            
vec_len=128           #length of vector generated by siamese vector
bs = 16               #batch size
delta = 2.            #constant for siamese loss
#@title import torch
import torch
import torch.nn as nn
from tqdm import tqdm
from torchaudio.transforms import MelScale, Spectrogram
#@title get model and optimizers : get_networks(),load(),build()
#Get models and optimizers
def get_networks(shape, load_model=False, path=None):
    if not load_model:
        gen,critic,siam = build()
    else:
        gen,critic,siam = load(path)
    print('Built networks')

    opt_gen = Adam(0.0001, 0.5)
    opt_disc = Adam(0.0001, 0.5)

    return gen,critic,siam, [opt_gen,opt_disc]

def get_networks_female(shape, load_model=False, path=None):
    if not load_model:
        gen_female,critic_female,siam_female = build()
    else:
        gen_female,critic_female,siam_female = load(path)
    print('Built networks')

    opt_gen_female = Adam(0.0001, 0.5)
    opt_disc_female = Adam(0.0001, 0.5)

    return gen_female,critic_female,siam_female, [opt_gen_female,opt_disc_female]

def load(path):
    gen = build_generator((hop,shape,1))
    siam = build_siamese((hop,shape,1))
    critic = build_critic((hop,3*shape,1))
    gen.load_weights(path+'/gen.h5')
    critic.load_weights(path+'/critic.h5')
    siam.load_weights(path+'/siam.h5')
    return gen,critic,siam
def build():
    gen = build_generator((hop,shape,1))
    siam = build_siamese((hop,shape,1))
    critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator
    return gen,critic,siam
#
#@title model : build_generator(),build_siamese(),build_critic(),conv2d(),deconv2d
def build_generator(input_shape):
    h,w,c = input_shape
    inp = Input(shape=input_shape)
  #downscaling
    g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)
    g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')
    g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))
    g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))
  #upscaling
    g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))
    g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)
    g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)
    return Model(inp,g6, name='G')

#Siamese Network
def build_siamese(input_shape): #評估兩個輸入樣本的相似度
    h,w,c = input_shape
    inp = Input(shape=input_shape)
    g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)
    g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)
    g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)
    g4 = Flatten()(g3)
    g5 = Dense(vec_len)(g4)
    return Model(inp, g5, name='S')

#Discriminator (Critic) Network
def build_critic(input_shape):
    h,w,c = input_shape
    inp = Input(shape=input_shape)
    g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)
    g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)
    g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)
    g4 = Flatten()(g3)
    g4 = DenseSN(1, kernel_initializer=init)(g4)
    return Model(inp, g4, name='C')
#Networks Architecture

init = tf.keras.initializers.he_uniform()

def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):
    if leaky:
        Activ = LeakyReLU(alpha=0.2)
    else:
        Activ = ReLU()
    if sn:
        d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)
    else:
        d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)
    if bnorm:
        d = BatchNormalization()(d)
    d = Activ(d)
    return d

def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):
    if up:
        u = UpSampling2D((1,2))(layer_input)
        u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)
    else:
        u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)
    if bnorm:
        u = BatchNormalization()(u)
    u = LeakyReLU(alpha=0.2)(u)
    if conc:
        u = Concatenate()([u,layer_res])
    return u

#@title ConvSN2D(),ConvSN2DTranspose(),DenseSN
#Adding Spectral Normalization to convolutional layers

from tensorflow.python.keras.utils import conv_utils
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import sparse_ops
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.ops import standard_ops
from tensorflow.python.eager import context
from tensorflow.python.framework import tensor_shape

def l2normalize(v, eps=1e-12):
    return v / (tf.norm(v) + eps)


class ConvSN2D(tf.keras.layers.Conv2D):

    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):
        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)
        self.power_iterations = power_iterations


    def build(self, input_shape):
        super(ConvSN2D, self).build(input_shape)

        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1

        self.u = self.add_weight(self.name + '_u',
            shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False
        )

    def compute_spectral_norm(self, W, new_u, W_shape):
        for _ in range(self.power_iterations):

            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
            new_u = l2normalize(tf.matmul(new_v, W))
            
        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
        W_bar = W/sigma

        with tf.control_dependencies([self.u.assign(new_u)]):
            W_bar = tf.reshape(W_bar, W_shape)

        return W_bar


    def call(self, inputs):
        W_shape = self.kernel.shape.as_list()
        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)
        outputs = self._convolution_op(inputs, new_kernel)

        if self.use_bias:
            if self.data_format == 'channels_first':
                    outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')
            else:
                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')
        if self.activation is not None:
            return self.activation(outputs)

        return outputs

class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):

    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):
        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)
        self.power_iterations = power_iterations


    def build(self, input_shape):
        super(ConvSN2DTranspose, self).build(input_shape)

        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1

        self.u = self.add_weight(self.name + '_u',
            shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False
        )

    def compute_spectral_norm(self, W, new_u, W_shape):
        for _ in range(self.power_iterations):

            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
            new_u = l2normalize(tf.matmul(new_v, W))
            
        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
        W_bar = W/sigma

        with tf.control_dependencies([self.u.assign(new_u)]):
            W_bar = tf.reshape(W_bar, W_shape)

        return W_bar

    def call(self, inputs):
        W_shape = self.kernel.shape.as_list()
        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)

        inputs_shape = array_ops.shape(inputs)
        batch_size = inputs_shape[0]
        if self.data_format == 'channels_first':
            h_axis, w_axis = 2, 3
        else:
            h_axis, w_axis = 1, 2

        height, width = inputs_shape[h_axis], inputs_shape[w_axis]
        kernel_h, kernel_w = self.kernel_size
        stride_h, stride_w = self.strides

        if self.output_padding is None:
            out_pad_h = out_pad_w = None
        else:
            out_pad_h, out_pad_w = self.output_padding

        out_height = conv_utils.deconv_output_length(height,
                                                    kernel_h,
                                                    padding=self.padding,
                                                    output_padding=out_pad_h,
                                                    stride=stride_h,
                                                    dilation=self.dilation_rate[0])
        out_width = conv_utils.deconv_output_length(width,
                                                    kernel_w,
                                                    padding=self.padding,
                                                    output_padding=out_pad_w,
                                                    stride=stride_w,
                                                    dilation=self.dilation_rate[1])
        if self.data_format == 'channels_first':
            output_shape = (batch_size, self.filters, out_height, out_width)
        else:
            output_shape = (batch_size, out_height, out_width, self.filters)

        output_shape_tensor = array_ops.stack(output_shape)
        outputs = K.conv2d_transpose(
            inputs,
            new_kernel,
            output_shape_tensor,
            strides=self.strides,
            padding=self.padding,
            data_format=self.data_format,
            dilation_rate=self.dilation_rate)

        if not context.executing_eagerly():
            out_shape = self.compute_output_shape(inputs.shape)
            outputs.set_shape(out_shape)

        if self.use_bias:
            outputs = tf.nn.bias_add(
                outputs,
                self.bias,
                data_format=conv_utils.convert_data_format(self.data_format, ndim=4))

        if self.activation is not None:
            return self.activation(outputs)
        return outputs  

class DenseSN(Dense):
    def build(self, input_shape):
        super(DenseSN, self).build(input_shape)

        self.u = self.add_weight(self.name + '_u',
            shape=tuple([1, self.kernel.shape.as_list()[-1]]), 
            initializer=tf.initializers.RandomNormal(0, 1),
            trainable=False)
        
    def compute_spectral_norm(self, W, new_u, W_shape):
        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))
        new_u = l2normalize(tf.matmul(new_v, W))
        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))
        W_bar = W/sigma
        with tf.control_dependencies([self.u.assign(new_u)]):
            W_bar = tf.reshape(W_bar, W_shape)
        return W_bar
        
    def call(self, inputs):
        W_shape = self.kernel.shape.as_list()
        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))
        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)
        rank = len(inputs.shape)
        if rank > 2:
            outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])
            if not context.executing_eagerly():
                shape = inputs.shape.as_list()
                output_shape = shape[:-1] + [self.units]
                outputs.set_shape(output_shape)
        else:
            inputs = math_ops.cast(inputs, self._compute_dtype)
            if K.is_sparse(inputs):
                outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)
            else:
                outputs = gen_math_ops.mat_mul(inputs, new_kernel)
        if self.use_bias:
            outputs = tf.nn.bias_add(outputs, self.bias)
        if self.activation is not None:
            return self.activation(outputs)
        return outputs

#@title preprocess : spectral_convergence(),GRAD(),normalize,denormalize(),prep(),deprep
#torch.set_default_tensor_type('torch.cuda.FloatTensor')

specobj = Spectrogram(n_fft=6*hop, win_length=6*hop, hop_length=hop, pad=0, power=2, normalized=True)
specfunc = specobj.forward
melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)
melfunc = melobj.forward

def spectral_convergence(input, target):
    return 20 * ((input - target).norm().log10() - target.norm().log10())
    
def GRAD(spec, transform_fn, samples=None, init_x0=None, maxiter=1000, tol=1e-6, verbose=1, evaiter=10, lr=0.003):

    spec = torch.Tensor(spec)
    samples = (spec.shape[-1]*hop)-hop

    if init_x0 is None:
        init_x0 = spec.new_empty((1,samples)).normal_(std=1e-6)
    x = nn.Parameter(init_x0)
    T = spec

    criterion = nn.L1Loss()
    optimizer = torch.optim.Adam([x], lr=lr)

    bar_dict = {}
    metric_func = spectral_convergence
    bar_dict['spectral_convergence'] = 0
    metric = 'spectral_convergence'

    init_loss = None
    with tqdm(total=maxiter, disable=not verbose) as pbar:
        for i in range(maxiter):
            optimizer.zero_grad()
            V = transform_fn(x)
            loss = criterion(V, T)
            loss.backward()
            optimizer.step()
            lr = lr*0.9999
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

            if i % evaiter == evaiter - 1:
                with torch.no_grad():
                    V = transform_fn(x)
                    bar_dict[metric] = metric_func(V, spec).item()
                    l2_loss = criterion(V, spec).item()
                    pbar.set_postfix(**bar_dict, loss=l2_loss)
                    pbar.update(evaiter)

    return x.detach().view(-1).cpu()

def normalize(S):
    return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)

def denormalize(S):
    return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db

def prep(wv,hop=192):
    S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())
    S = librosa.power_to_db(S)-ref_level_db
    return normalize(S)

def deprep(S):
    S = denormalize(S)+ref_level_db
    S = librosa.db_to_power(S)
    wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2000, evaiter=10, tol=1e-8)
    return np.array(np.squeeze(wv))
  
def melspecfunc(waveform):
    specgram = specfunc(waveform)
    mel_specgram = melfunc(specgram)
    return mel_specgram

#Build models and initialize optimizers
#If load_model=True, specify the path where the models are saved

###male###
gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=True, path='C:/Users/User/MELGANVC-0.5955072-0.4417478-0.0938403')
###female###
gen_female,critic_female,siam_female, [opt_gen_female,opt_disc_female] = get_networks_female(shape, load_model=True, path='C:/Users/User/MELGANVC-0.5599844-0.5263780-0.0959562')

#@title covert data,save result : spacass(),chopspec(),towave()
#After Training, use these functions to convert data with the generator and save the results

#Assembling generated Spectrogram chunks into final Spectrogram
def specass(a,spec):
  but = False
  con = np.array([])
  nim = a.shape[0]
  for i in range(nim-1):
    im = a[i]
    im = np.squeeze(im)
    if not but:
      con=im
      but=True
    else:
      con = np.concatenate((con,im), axis=1)
  diff = spec.shape[1]-(nim*shape)
  a = np.squeeze(a)
  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)
  return np.squeeze(con)

#Splitting input spectrogram into different chunks to feed to the generator
def chopspec(spec):
  dsa=[]
  for i in range(spec.shape[1]//shape):
    im = spec[:,i*shape:i*shape+shape]
    im = np.reshape(im, (im.shape[0],im.shape[1],1))
    dsa.append(im)
  imlast = spec[:,-shape:]
  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))
  dsa.append(imlast)
  return np.array(dsa, dtype=np.float32)

#Converting from source Spectrogram to target Spectrogram
def towave(spec, show=False):
  specarr = chopspec(spec)
  print(specarr)
  a = specarr
  print('Generating...')
  ab = gen(a, training=False)
  print('Assembling and Converting...')
  #a = specass(a,spec)W
  ab = specass(ab,spec)
  #awv = deprep(a)
  abwv = deprep(ab)
  print('Saving...')
  '''pathfin = f'{path}/{name}'
  if not os.path.isdir(pathfin): 
    os.mkdir(pathfin)
  sf.write(pathfin+'/AB.wav', abwv, sr)
  sf.write(pathfin+'/A.wav', awv, sr)
  print('Saved WAV!')'''
  #IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))
  #IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))

  return abwv

def towave_female(spec, show=False):
  specarr = chopspec(spec)
  print(specarr.shape)
  a = specarr
  print('Generating...')
  ab = gen_female(a, training=False)
  print('Assembling and Converting...')
  #a = specass(a,spec)
  ab = specass(ab,spec)
  #awv = deprep(a)
  abwv = deprep(ab)
  print('Saving...')
  '''pathfin = f'{path}/{name}'
  if not os.path.isdir(pathfin): 
    os.mkdir(pathfin)
  sf.write(pathfin+'/AB.wav', abwv, sr)
  sf.write(pathfin+'/A.wav', awv, sr)
  print('Saved WAV!')'''
  #IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))
  #IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))

  return abwv

from moviepy.editor import *

def upload_file(request): #加上進度條 還要改
    if request.method == 'POST' :
        myfile = request.FILES['myfile']
        myfile_name = request.FILES['myfile'].name
        myfile_name = str(myfile_name)
        #!!儲存image!!#
        myimage = request.FILES['myimage']
        myimage_name = request.FILES['myimage'].name
        myimage_name = str(myimage_name)
        ###儲存mp4.image###
        wav = UserSong.objects.create(audio_file = myfile,give_image = myimage)
        wav.save()
        print(myfile_name)
        ###mp4切成mp3###
        del_end_myfile_name = str(myfile_name.strip('.mp4'))
        print(del_end_myfile_name)
        myfile_process_mp4 = os.path.join('media/'+myfile_name)
        video=VideoFileClip(myfile_process_mp4)
        video.audio.write_audiofile('media/'+del_end_myfile_name+".mp3")
        ###存mp3###
        video_out = UserSong.objects.create(audio_file = del_end_myfile_name+".mp3")
        myfile_process_mp3 = os.path.join('media/'+del_end_myfile_name+".mp3")
        ###取得###
        #myfile_change = UserSong.objects.get(audio_file = del_end_myfile_name+".mp3")
        #myfile_change = myfile_change.audio_file
        ###身體轉換###
        """source_image = imageio.imread('./media/give_image/'+myimage_name)
        driving_video = imageio.get_reader('./media/'+myfile_name)
        source_image = resize(source_image, (384, 384))[..., :3]
        driving_video = [resize(frame, (384, 384))[..., :3] for frame in driving_video]
        print("aaaa")
        generator, region_predictor, avd_network = load_checkpoints(config_path='upload_wav/config/ted384.yaml',checkpoint_path='upload_wav/checkpoints/ted384.pth')
        print("bbbb")
        predictions = make_animation(source_image, driving_video, generator, region_predictor, avd_network, animation_mode='avd')
        print("cccc")
        #save resulting video
        imageio.mimsave('media/'+"output_"+del_end_myfile_name+".mp4", [img_as_ubyte(frame) for frame in predictions])
        UserSong.objects.get_or_create(audio_file="output_"+del_end_myfile_name+".mp4")###
        print("dddd")"""
        ###套臉轉換###
        source_image = imageio.imread('./media/give_image/'+myimage_name)
        reader = imageio.get_reader('./media/'+myfile_name)
        fps = reader.get_meta_data()['fps']
        source_image = resize(source_image, (384, 384))[..., :3]
        driving_video = []
        try:
            for im in reader:            
                driving_video.append(im)
        except RuntimeError:              
            pass
        reader.close()  
        driving_video = [resize(frame, (384, 384))[..., :3] for frame in driving_video]

        print("aaaa")
        generator, kp_detector =  load_checkpoints(config_path='video_exfile_face/config/vox-adv-256.yaml',checkpoint_path='video_exfile_face/checkpoint/vox-adv-cpk.pth.tar')
        print("bbbb")
        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True)
        print("cccc")
        #save resulting video
        imageio.mimsave('media/'+"output_"+del_end_myfile_name+".mp4", [img_as_ubyte(frame) for frame in predictions], fps=fps)
        UserSong.objects.get_or_create(audio_file="output_"+del_end_myfile_name+".mp4")###
        print("dddd")        
        ###選擇轉換類型###
        ###載入檔案給模型###
        changetype = request.POST.get('changetype')
        if changetype == 'men':
            wv, sr = librosa.load(myfile_process_mp3, sr=16000)  #Load waveform
            ###pre process###
            '''Noise Suppression '''
            reduced_noise = nr.reduce_noise(y = wv, sr=sr, thresh_n_mult_nonstationary=2,stationary=False)
            ###pre process end ###
            print(wv.shape)
            print("men")
            speca = prep(reduced_noise) 
            abwv = towave(speca)  #
        else :
            wv, sr = librosa.load(myfile_process_mp3, sr=16000)  #Load waveform
            ###pre process###
            '''Noise Suppression '''
            reduced_noise = nr.reduce_noise(y = wv, sr=sr, thresh_n_mult_nonstationary=2,stationary=False)
            ###pre process end ###
            print(wv.shape)
            print("female")
            speca = prep(reduced_noise) 
            abwv = towave_female(speca)  #        
        ####避免資料庫重複### get_or_create
        ###存轉換###
        twz = UserSong.objects.get_or_create(audio_file="output_"+del_end_myfile_name+".wav")
        #twz.save()
        ###sf莫名只能存wav格式###
        sf.write("media/output_"+del_end_myfile_name+".wav", abwv, sr)
        ###輸出用###
        request.session['OUTPUT_RESULT'] = "output_"+del_end_myfile_name+".wav"# 設定讀取

        return HttpResponseRedirect('/show_result')
    #if request.method == 'POST':
    if request.POST.get('upload',False):
        #print(uploaded_file.name)
        form = UserSongForm(request.POST, request.FILES)
        if form.is_valid():
            #audio_file_output = uploaded_file.read()
            audio_file = form.cleaned_data['audio_file']
            wav = UserSong.objects.create(audio_file = audio_file)
            wav.save()    #存database

            #print(str(audio_file)) #測試ffmpeg取得檔案用
            #media = "media/" 
            #media = media+str(audio_file)
            #print(media) #測試

            #
            #fs = FileSystemStorage() #same as save()
            #fs.save(uploaded_file.name,uploaded_file) #equal but it is not overwrite ori file
                
            #handle_uploaded_file(request.FILES.get('audio_file')) #存本機
                    
            return HttpResponseRedirect('/upload')

    else:
        form = UserSongForm()
    return render(request,'upload_doc.html', {'form': form})

import scipy.io
import scipy.io.wavfile
import matplotlib.pyplot as plt
import io,urllib, base64,os
import numpy as np
import wave
import librosa
import librosa.display

def show_result(request): 
    if request.method == "GET":
        if 'OUTPUT_RESULT' in request.session:
            output_result = request.session['OUTPUT_RESULT'] # 讀取lucky_number
            print(output_result)
            output_result_video=str(output_result)
            output_result_video = str(output_result_video.strip('.wav'))
            output_result_video = output_result_video+".mp4"
            print(output_result_video+"|check")
            if UserSong.objects.filter(audio_file = str(output_result)).exists():
                show =  UserSong.objects.get(audio_file = str(output_result))
                #demo exchanged video
                show_video = UserSong.objects.get(audio_file = str(output_result_video))
                print(show)
                print(show_video)
                ###show image of wave###
                wavedata = os.path.join('media/',output_result)
                ###show amplitude1###
                f = plt.figure(figsize=(10,4))
                ax = f.add_subplot(121)
                sampleRate, audioBuffer = scipy.io.wavfile.read(wavedata)
                duration = len(audioBuffer)/sampleRate
                time = np.arange(0,duration,1/sampleRate) #time vector
                ######
                ax.plot(time,audioBuffer)
                ax.set_xlabel('Time [s]')
                ax.set_ylabel('Amplitude')
                ax.title.set_text("Amplitude")
                ###show mel-spectrum2###
                ax2 = f.add_subplot(122)
                scale, sr = librosa.load(wavedata,sr=16000)
                mel_spectrogram = librosa.feature.melspectrogram(scale, sr=sr, n_fft=2048, hop_length=512, n_mels=10)
                ##plt.figure(figsize=(25,10))##dishow the first one image
                librosa.display.specshow(mel_spectrogram, 
                                            x_axis="time",
                                            y_axis="mel", 
                                            sr=sr)
                plt.colorbar(format="%+2.f")
                ax2.title.set_text("Mel-spectrogram")
                ###show in webpage###
                fig = plt.gcf()
                #convert graph into dtring buffer and then we convert 64 bit code into image
                buf = io.BytesIO()
                fig.savefig(buf,format='png')
                buf.seek(0)
                string = base64.b64encode(buf.read())
                uri =  urllib.parse.quote(string)
    return render(request,'show.html',{'show':show,'show_video':show_video,'data':uri})

 